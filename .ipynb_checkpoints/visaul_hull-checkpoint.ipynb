{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0fb8b5f-ae7a-4152-80da-9d22784482b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using resized images in ../capston2/dataset/omni3d/backpack_016/images...\n",
      "Reading camera 200/200\n",
      "Generating ellipse path from 200 camera infos ...\n",
      "theta[0] 0.0\n",
      "the sparse id is 9, with 9 frames\n",
      "the camera center is: tensor([ 0.0376, -0.0233, -0.0062], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1417.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual hull is Okay, with 29104 points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:00<00:00, 1575.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual hull is Okay, with 15445 points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from argparse import Namespace\n",
    "\n",
    "import camtools as ct\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from scene.dataset_readers import sceneLoadTypeCallbacks\n",
    "from utils.camera_utils import cameraList_from_camInfos\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "from typing import NamedTuple\n",
    "from torchvision import transforms\n",
    "\n",
    "class SceneInfo(NamedTuple):\n",
    "    Ks: list\n",
    "    Ts: list\n",
    "    images: list\n",
    "    masks: list\n",
    "    \n",
    "\n",
    "def fov2focal(fov, pixels):\n",
    "    return pixels / (2 * math.tan(fov / 2))\n",
    "\n",
    "def points2homopoints(points):\n",
    "    assert points.shape[-1] == 3\n",
    "    bottom = torch.ones_like(points[...,0:1])\n",
    "    return torch.cat([points, bottom], dim=-1)\n",
    "\n",
    "def batch_projection(Ks, Ts, points):\n",
    "    '''\n",
    "    Ks: B, 3, 3\n",
    "    Ts: B, 4, 4\n",
    "    points: B, N, 3\n",
    "    '''\n",
    "    pre_fix = points.shape[:-1] # [100, 100]\n",
    "    points = points.reshape(-1, 3) # [M, 3]\n",
    "\n",
    "    Ts = torch.stack(Ts, dim=0) # [N, 4, 4]\n",
    "    Ks = torch.stack(Ks, dim=0).to(Ts.device) # [N, 3, 3]\n",
    "    camera_num = Ks.shape[0]\n",
    "    homopts = points2homopoints(points) # [M, 4]\n",
    "    # world to camera # [N, M, 4] @ [N, 4, 4] = [N, M, 4]\n",
    "    homopts_cam = torch.bmm(homopts.unsqueeze(0).repeat_interleave(Ts.shape[0], dim=0), Ts.transpose(1,2)) \n",
    "    # camera to image space  # [N, M, 4] @ [N, 4, 3] = [N, M, 3]\n",
    "    homopts_img = torch.bmm(homopts_cam[...,:3], Ks.transpose(1,2))\n",
    "    # normalize\n",
    "    homopts_img = homopts_img / (homopts_img[...,2:] + 1e-6)\n",
    "    # reshape back\n",
    "    homopts_img = homopts_img.reshape(camera_num, *pre_fix, 3)\n",
    "    homopts_cam = homopts_cam.reshape(camera_num, *pre_fix, 4)\n",
    "    return homopts_img[...,0:2], homopts_cam[...,2]\n",
    "\n",
    "def query_from_list_with_list(listA: list, listB: list):\n",
    "    '''\n",
    "    listA: [1, 2, 3]\n",
    "    listB: [3, 2, 1]\n",
    "    return: [2, 1, 0]\n",
    "    '''\n",
    "    return [listB[i] for i in listA]\n",
    "\n",
    "def simple_resize_image(img, size):\n",
    "    return transforms.Resize(size, antialias=True)(img)\n",
    "\n",
    "def get_visual_hull(N, bbox, scene_info, cam_center):\n",
    "    pcs = []\n",
    "    color = []\n",
    "    all_pts = []\n",
    "    Ks = scene_info.Ks\n",
    "    Ts = scene_info.Ts\n",
    "    images = scene_info.images\n",
    "    masks = scene_info.masks\n",
    "\n",
    "    [xs, ys, zs], [xe, ye, ze] = bbox[0], bbox[1]\n",
    "\n",
    "    # please note that in vasedeck, the images are not same size, for simplify, just resize them\n",
    "    new_images = []\n",
    "    new_masks = []\n",
    "    img_size = images[0].shape[1:]\n",
    "    for image, mask in zip(images, masks):\n",
    "        new_images.append(simple_resize_image(image, img_size))\n",
    "        new_masks.append(simple_resize_image(mask, img_size))\n",
    "\n",
    "    images = torch.stack(new_images) # N C H W\n",
    "    masks = torch.stack(new_masks) # N 1 H W\n",
    "\n",
    "    for h_id in trange(N):\n",
    "        i, j = torch.meshgrid(torch.linspace(xs, xe, N).cuda(),\n",
    "                              torch.linspace(ys, ye, N).cuda())\n",
    "        i, j = i.t(), j.t()\n",
    "        pts = torch.stack([i, j, torch.ones_like(i).cuda()], -1)\n",
    "        pts[...,2] = h_id / N * (ze - zs) + zs # 100, 100, 3\n",
    "\n",
    "        # shift the pts to be centered at the camera center\n",
    "        pts[...,0] += cam_center[0]  # note the order, [x, y, z], width, height, depth\n",
    "        pts[...,1] += cam_center[1]\n",
    "        pts[...,2] += cam_center[2]\n",
    "\n",
    "        all_pts.append(pts)\n",
    "\n",
    "        # now we have the pts, we need to project them to the image plane\n",
    "        # batched projection\n",
    "        uv, z = batch_projection(Ks, Ts, pts) # [N, 100, 100, 2], [N, 100, 100]\n",
    "        valid_z_mask = z > 0\n",
    "        valid_x_y_mask = (uv[...,0] > 0) & (uv[...,0] < cam_info.image_width) & (uv[...,1] > 0) & (uv[...,1] < cam_info.image_height)\n",
    "        valid_pt_mask = valid_z_mask & valid_x_y_mask\n",
    "\n",
    "        # simple resize the uv to [-1, 1]\n",
    "        uv[...,0] = uv[...,0] / cam_info.image_width * 2 - 1\n",
    "        uv[...,1] = uv[...,1] / cam_info.image_height * 2 - 1\n",
    "\n",
    "        # now we have the uv, we use grid_sample to sample the image to get the color\n",
    "        result = F.grid_sample(images.float(), uv, padding_mode='zeros', align_corners=False).permute(0, 2, 3, 1) # N, 100, 100, 3\n",
    "        # sample mask\n",
    "        result_mask = F.grid_sample(masks.float(), uv, padding_mode='zeros', align_corners=False).permute(0, 2, 3, 1) # N, 100, 100, 1\n",
    "\n",
    "        valid_pt_mask = result_mask.squeeze() > 0 & valid_pt_mask\n",
    "\n",
    "        pcs.append(valid_pt_mask.float().sum(0) >= (images.shape[0] - 1)) # [100, 100]\n",
    "        color.append(result.mean(0)) # [100, 100, 3]\n",
    "    \n",
    "    pcs = torch.stack(pcs, -1)\n",
    "    color = torch.stack(color, -1)\n",
    "\n",
    "    r, g, b = color[:, :, 0], color[:, :, 1], color[:, :, 2]\n",
    "    idx = torch.where(pcs > 0)\n",
    "\n",
    "    color = torch.stack((r[idx] * 255, g[idx] * 255, b[idx] * 255), -1)\n",
    "\n",
    "    idx = torch.stack([idx[1], idx[0], idx[2]], -1) # note the order is hwz -> xyz\n",
    "    # turn the idx to the point position used in batch_projection\n",
    "    idx = idx.float() / N\n",
    "    idx[...,0] = idx[...,0] * (xe - xs) + xs + cam_center[0]\n",
    "    idx[...,1] = idx[...,1] * (ye - ys) + ys + cam_center[1]\n",
    "    idx[...,2] = idx[...,2] * (ze - zs) + zs + cam_center[2]\n",
    "\n",
    "    print(\"visual hull is Okay, with {} points\".format(idx.shape[0]))\n",
    "    # we get the point cloud, use open3d to visualize it\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    \n",
    "    pcd.points = o3d.utility.Vector3dVector(idx.cpu().numpy())\n",
    "    pcd.colors = o3d.utility.Vector3dVector(color.cpu().numpy() / 255)\n",
    "    # get bbox\n",
    "    bbox = pcd.get_axis_aligned_bounding_box()\n",
    "    return pcd, bbox\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    sys.argv = ['visaul_hull.ipynb','--data_dir','../capston2/dataset/omni3d/backpack_016',\"--cube_size\",'4.0',\"--voxel_num\",'200',\n",
    "               '--sparse_id',\"9\",'--reso','1','--not_vis',\"--cube_size_shift_x\",'0.0',\"--cube_size_shift_y\",'0.0',\n",
    "                \"--cube_size_shift_z\",'0.0']\n",
    "    parser = argparse.ArgumentParser(description='generate k views covering object')\n",
    "    parser.add_argument('--data_dir', type=str,  help='data directory, we only support colmap type data, kitchen, garden')\n",
    "    parser.add_argument(\"--cube_size\", type=float, help=\"size of the cube in meters\")\n",
    "    parser.add_argument(\"--voxel_num\", type=int,  help=\"size of a voxel in meters\")\n",
    "    parser.add_argument('--sparse_id', type=int,  help='sparse id')\n",
    "    parser.add_argument('--reso', type=int, help='the resolution of image, 1 for omni3d, 4 or 8 for mip360')\n",
    "    parser.add_argument('--not_vis', action='store_true', help='whether vis the visual hull, is enable, not vis')\n",
    "    parser.add_argument(\"--cube_size_shift_x\", type=float,  help=\"shift sizex of the cube in meters\")\n",
    "    parser.add_argument(\"--cube_size_shift_y\", type=float,  help=\"shift sizey of the cube in meters\")\n",
    "    parser.add_argument(\"--cube_size_shift_z\", type=float,  help=\"shift sizez of the cube in meters\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    extra_opts = Namespace()\n",
    "    extra_opts.sparse_view_num = -1\n",
    "    extra_opts.resolution = args.reso\n",
    "    extra_opts.use_mask = True\n",
    "    extra_opts.data_device = 'cuda'\n",
    "    extra_opts.init_pcd_name = 'origin'\n",
    "    extra_opts.white_background = False\n",
    "\n",
    "    # load the camera parameters\n",
    "    # we assume that the camera parameters are stored in the data_dir\n",
    "    scene_info = sceneLoadTypeCallbacks[\"Colmap\"](args.data_dir, 'images', False, extra_opts=extra_opts) \n",
    "    camlist = cameraList_from_camInfos(scene_info.train_cameras, 1.0, extra_opts)\n",
    "\n",
    "    # if sparse id is not zero, we only use given frames to construct the visual hull\n",
    "    if args.sparse_id >= 0:\n",
    "        selected_id = np.loadtxt(os.path.join(args.data_dir, f\"sparse_{str(args.sparse_id)}.txt\"), dtype=np.int32)\n",
    "        print(\"the sparse id is {}, with {} frames\".format(args.sparse_id, len(selected_id)))\n",
    "        assert args.sparse_id == len(selected_id)\n",
    "    else:\n",
    "        selected_id = np.arange(len(camlist))\n",
    "\n",
    "    # get all camera locations to recenter the scene\n",
    "    cam_locations = []\n",
    "    cam_rotations = []\n",
    "    cam_T = []\n",
    "    Ts = []\n",
    "    Ks = []\n",
    "    images = []\n",
    "    masks = []\n",
    "    for cam_info in camlist:\n",
    "        cam_locations.append(cam_info.camera_center)\n",
    "        cam_rotations.append(cam_info.R)\n",
    "        cam_T.append(cam_info.T)\n",
    "        Ts.append(cam_info.world_view_transform.T)\n",
    "        fx = fov2focal(cam_info.FoVx, cam_info.image_width)\n",
    "        fy = fov2focal(cam_info.FoVy, cam_info.image_height)\n",
    "        Ks.append(torch.tensor([[fx, 0, cam_info.image_width/2], [0, fy, cam_info.image_height/2], [0, 0, 1]]))\n",
    "        images.append(cam_info.original_image)\n",
    "        masks.append(cam_info.mask)\n",
    "\n",
    "    # in this time, we already have the camera parameters\n",
    "    # first, we get the cemera locations center\n",
    "    cam_center = torch.stack(cam_locations).mean(0)\n",
    "    print('the camera center is:', cam_center)\n",
    " \n",
    "    Ks = query_from_list_with_list(selected_id, Ks)\n",
    "    Ts = query_from_list_with_list(selected_id, Ts)\n",
    "    images = query_from_list_with_list(selected_id, images)\n",
    "    masks = query_from_list_with_list(selected_id, masks)\n",
    "\n",
    "    scene_info = SceneInfo(Ks, Ts, images, masks)\n",
    "    Ks_clone = copy.deepcopy(Ks)\n",
    "\n",
    "    bx = args.cube_size\n",
    "    init_bbox = [[args.cube_size_shift_x-bx, args.cube_size_shift_y-bx, args.cube_size_shift_z-bx], \n",
    "                 [args.cube_size_shift_x+bx, args.cube_size_shift_y+bx, args.cube_size_shift_z+bx]]\n",
    "    # we run the get_visual_hull twice, first to get the bound, second to get the visual hull\n",
    "    pcd, bbox = get_visual_hull(args.voxel_num, init_bbox, scene_info, cam_center)\n",
    "    \n",
    "    # since we get the bound, we use this bound to better recon\n",
    "    # we use the center of the bound as the center of the scene\n",
    "    # please note that the bbox may need bigger, since the camera may not cover the whole scene\n",
    "    bbox_min = bbox.get_min_bound()\n",
    "    bbox_max = bbox.get_max_bound()\n",
    "    # Calculate the center point of the original bounding box\n",
    "    center = (bbox_min + bbox_max) / 2\n",
    "    # Calculate the extents of the original bounding box\n",
    "    extents = bbox_max - bbox_min\n",
    "    # Calculate the scale factor to increase the size by 20% (1.2 times)\n",
    "    scale_factor = 2\n",
    "    # Calculate the scaled extents\n",
    "    scaled_extents = extents * scale_factor\n",
    "    # Calculate the new minimum and maximum points of the enlarged bounding box\n",
    "    enlarged_bbox_min = center - scaled_extents / 2\n",
    "    enlarged_bbox_max = center + scaled_extents / 2\n",
    "\n",
    "    pcd, bbox_new = get_visual_hull(64, [enlarged_bbox_min, enlarged_bbox_max], scene_info, [0,0,0])\n",
    "    # save the pointcloud\n",
    "    if args.sparse_id >= 0:\n",
    "        o3d.io.write_point_cloud(os.path.join(args.data_dir, f\"visual_hull_{str(args.sparse_id)}.ply\"), pcd)\n",
    "    else:\n",
    "        o3d.io.write_point_cloud(os.path.join(args.data_dir, \"visual_hull_full.ply\"), pcd)\n",
    "\n",
    "    if not args.not_vis:\n",
    "        Ts = np.array([i.cpu().numpy() for i in Ts])\n",
    "        Ks = np.array(Ks_clone)\n",
    "        cameras = ct.camera.create_camera_frustums(Ks, Ts, highlight_color_map={0: [1, 0, 0], -1: [0, 1, 0]})\n",
    "        # build LineSet to represent the coordinate\n",
    "        world_coord = o3d.geometry.LineSet()\n",
    "        world_coord.points = o3d.utility.Vector3dVector(np.array([[0, 0, 0], [2, 0, 0], \n",
    "                                                                [0, 0, 0], [0, 2, 0], \n",
    "                                                                [0, 0, 0], [0, 0, 2]]))\n",
    "        world_coord.lines = o3d.utility.Vector2iVector(np.array([[0, 1], [0, 3], [0, 5]]))\n",
    "        # X->red, Y->green, Z->blue\n",
    "        world_coord.colors = o3d.utility.Vector3dVector(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n",
    "        \n",
    "        pcdo = o3d.io.read_point_cloud(os.path.join(args.data_dir, \"sparse/0/points3D.ply\"))\n",
    "\n",
    "        # init viewer\n",
    "        # o3d.visualization.webrtc_server.enable_webrtc()\n",
    "        viewer = o3d.visualization.Visualizer()\n",
    "        viewer.create_window()\n",
    "        viewer.add_geometry(cameras)\n",
    "        viewer.add_geometry(pcd)\n",
    "        viewer.add_geometry(world_coord)\n",
    "    \n",
    "        opt = viewer.get_render_option()\n",
    "        opt.background_color = np.asarray([0.5, 0.5, 0.5])\n",
    "        viewer.run()\n",
    "        viewer.destroy_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e16f758-36df-4ce4-847b-9f98f2926c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] GLFW Error: X11: Failed to open display localhost:10.0\u001b[0;m\n",
      "\u001b[1;33m[Open3D WARNING] Failed to initialize GLFW\u001b[0;m\n",
      "\u001b[1;33m[Open3D WARNING] [DrawGeometries] Failed creating OpenGL window.\u001b[0;m\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# PLY 파일 경로\n",
    "ply_file_path = \"../capston2/dataset/omni3d/backpack_016/visual_hull_9.ply\"  # 여기에 본인의 PLY 파일 경로를 입력하세요.\n",
    "\n",
    "# PLY 파일 읽기\n",
    "point_cloud = o3d.io.read_point_cloud(ply_file_path)\n",
    "\n",
    "# 점구름 시각화\n",
    "o3d.visualization.draw_geometries([point_cloud])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef6d2d-591d-4a1b-87d8-faf1e8298b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
